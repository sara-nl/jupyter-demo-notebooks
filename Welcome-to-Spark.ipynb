{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img style=\"float: right\" src=\"images/surfsara.png\">\n",
    "<br/>\n",
    "<hr style=\"clear: both\" />\n",
    "\n",
    "# Spark Structured API: DataFrames and SQL - basics\n",
    "In this notebook we will look at Spark's Structured API. We will see how you can use DataFrames and SQL to perform common data processing operations.\n",
    "\n",
    "This notebook relies on some datasets that we need to download first. **Please run the following cell to make sure all data is available.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to create a `SparkSession` object. This will be the entry point to our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `SparkSession` object is the entry point for our Spark cluster. We can use a `SparkSession` to create DataFrames, as we will soon see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames from Python collections\n",
    "We can create a DataFrame from an existing Python collection, such as a list or a dictionary. In addition to the collection itself we will also describe (part of) the structure of the data by naming the columns. Additionally, we can specify the data types of the columns. However, in this case we will let Spark infer this automatically.\n",
    "\n",
    "First, a list of tuples in Python is created, called `phone_stock`. Next, we create a list called `columns` that contain the name of all columns of the DataFrame. Then we use these two lists as input for [`createDataFrame`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession.createDataFrame). The result is the DataFrame `phone_df`. Next we print the type of both `phone_stock` and `phone_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_stock = [\n",
    "    ('iPhone 6', 'Apple', 6, 549.00),\n",
    "    ('iPhone 6s', 'Apple', 5, 585.00),\n",
    "    ('iPhone 7', 'Apple', 11, 739.00),\n",
    "    ('Pixel', 'Google', 8, 859.00),\n",
    "    ('Pixel XL', 'Google', 2, 959.00),\n",
    "    ('Galaxy S7', 'Samsung', 10, 539.00),\n",
    "    ('Galaxy S6', 'Samsung', 5, 414.00),\n",
    "    ('Galaxy A5', 'Samsung', 7, 297.00),\n",
    "    ('Galaxy Note 7', 'Samsung', 0, 841.00)\n",
    "]\n",
    "\n",
    "columns = ['model', 'brand', 'stock', 'unit_price']\n",
    "\n",
    "phone_df = spark.createDataFrame(phone_stock, columns)\n",
    "\n",
    "print('the type of phone_stock: ' + str(type(phone_stock)))\n",
    "print('the type of phone_df: ' + str(type(phone_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `phone_stock` variable is a Python `list`, kept in memory. Using `createDataFrame`, it is converted into a DataFrame, which is now located on our Spark cluster. It is important to distinguish between data available locally (driver-side), such as `phone_stock`, and data available on the cluster (cluster-side), such as `phone_df`.<br>\n",
    "\n",
    "Although data can be downloaded from the cluster to the driver, and uploaded from the driver to the cluster, this is often a costly operation. In order to reduce computation time, it is important to know where our data 'lives', and how Spark operation affect the location of our data. We will come back to this issue later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data into your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see a few rows of a DataFrame we use [`show()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show). By default it shows 20 rows, but you can give the desired number of rows that you want to see as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`collect()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.collect) _action_ returns all data from a DataFrame to the driver. Notice that the result is a Python `list` containing `Row` objects. In turn, each `Row` object consists of pairs of column names and attribute values.\n",
    "\n",
    "**Note**: `collect` is a costly operation. Each executor in the cluster will return its part of the DataFrame to the driver in its entirety. This is fine for small DataFrames, such as on the one we are using, but can cause serious problems on larger DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phones = phone_df.collect()\n",
    "all_phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working directly with a list of row objects is cumbersome. To work directly with data on the driver's side, we usually convert the Spark DataFrame to a `pandas` DataFrame. [`pandas`](https://pandas.pydata.org/) is a data processing library that allows us to manipulate tabular table. It is suitable for processing that isn't too intensive and data that isn't too large to fit into local memory (otherwise, why would we want to use Spark?).\n",
    "\n",
    "Spark DataFrames have a [`toPandas()`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas) action defined on them, that will pull all data to the driver and convert it to a `pandas` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods to inspect the structure of a DataFrame: `printSchema`, `schema` and `describe`. [`printSchema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema) is especially useful with complicated nested structures, because it provides a human-readable form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all columns are listed, together with their type and a boolean value that indicates whether the value for that column can be `NULL`. Although this DataFrame does not contain nested structures, we will see a more involved example later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema's can also be listed programmatically. By calling [`schema`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema) we get to see the structure of the DataFrame in Spark's internal types. It is possible to define a schema in code by making use of these types, although we won't do this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `schema`'s `fields` attribute will provide a more readable way to inspect the DataFrame's structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.schema.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`describe`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) will compute summary statistics for numeric and string columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "Now that we have our data in a DataFrame, we want to use it to manipulate the data. Let's start by selecting subsets of the data: specific columns and/or rows.\n",
    "\n",
    "### Selecting columns\n",
    "\n",
    "Often we are not interested in all the columns of our data. DataFrames make it very easy to select a subset of the data by using the [`select`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) method. Realise that we are not modifying the original DataFrame, but creating a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the model column\n",
    "model_df = phone_df.select('test')\n",
    "model_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also rename a column by using [`expr`](https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark.sql.functions.expr). It will take a string as input describing the original column name and its new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "mymodel_df = phone_df.select('brand', expr('model as mymodel'))\n",
    "mymodel_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select both the brand and model columns\n",
    "bm_df = phone_df.select(['brand', 'model'])\n",
    "bm_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Columns specifications\n",
    "In the previous examples we have used various _column specifications_ for selecting data. For example, the following column specifications for selecting the `brand` and `model` columns are all equivalent:\n",
    "```\n",
    "bm_df = phone_df.select('brand', 'model')\n",
    "bm_df = phone_df.select(['brand', 'model'])\n",
    "bm_df = phone_df.select([phone_df.brand, phone_df.model])\n",
    "bm_df = phone_df.select(phone_df['brand'], phone_df['model'])\n",
    "```\n",
    "Although the first two are shorter, we will sometimes need to use the last two column specifications for more complex queries, and to resolve any ambiguities for Spark's parser. In fact, the Spark documentation states that the last column specification is preferred, although you are free to use the specification you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "Select the `model` and `stock` columns from `phone_df`, with whatever column specification you prefer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Select the model and stock columns\n",
    "\n",
    "ms_df = phone_df.<FILL_IN>\n",
    "ms_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering rows\n",
    "\n",
    "We can filter specific rows by using the DataFrame [`filter`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.filter) method. Please note that the [`where`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where) method is an alias for `filter`. As with the column specifications for the `select` method, there are several ways of filtering with the same query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select rows with phones from Google\n",
    "google_df = phone_df.filter(phone_df['brand'] == 'Google')\n",
    "google_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows with phones from Google, this time with a query string. Note the double quotes to specify a string\n",
    "google_df = phone_df.filter('brand = \"Google\"')\n",
    "google_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Select the rows with `unit_price` less than 550.00. Try a different query specification, as was shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "cheap_df = phone_df.filter(<FILL IN>)\n",
    "cheap_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple filter conditions can be specified using Python's [bitwise operators](https://docs.python.org/3/library/stdtypes.html#bitwise-operations-on-integer-types), such as `|` (or) and `&` (and). Of course, we are not manipulating bits. Instead, think of the bitwise operators as applying a boolean operation on each pair of elements between two columns.\n",
    "\n",
    "Also, please be aware that due to Python's operator precedence rules, individual expressions must be wrapped in enclosing brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.filter((phone_df.brand == 'Apple') | (phone_df.brand == 'Google')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordering rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`orderBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy) method or its alias [`sort`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderby) to sort data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.orderBy('unit_price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we use a chain of DataFrame methods that are very similar to the SQL query language used for certain databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.select(\"model\", \"unit_price\").where(\"brand='Apple'\").orderBy('stock', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way of doing the same as the cell above is using `phone_df[\"brand\"]` in the where clause. This is longer to type but intuitively more clear and easier to read. There is no ambiguity for the Spark parser with this notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.select(\"model\", \"brand\", \"unit_price\").where(phone_df[\"brand\"]==\"Apple\").orderBy('stock', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Select all phones with a unit price larger than 300 and of which there are more than two in stock. Display the remaining phones, ordered by brand, followed by stock. Use whatever column specification syntax you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column operations\n",
    "Spark has a large number of functions that can be applied to columns. Since Spark data frames are immutable, any changes to columns need to be added to the data frame as a new column. We do this with the Spark [`withColumn`](http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn) method. It takes the name of the new column as its first argument, and as its second argument an expression to generate the data for the column.\n",
    "\n",
    "Let's look at an example with some column arithmetic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.withColumn('times_two', phone_df['stock'] * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df.withColumn('squared_stock', phone_df['stock'] ** 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "The second argument of `withColumn` can be any expression, meaning that you can combine two or more columns as well. Create a new colum on the `phone_df` data frame called `expected_revenue`, where you multiply the unit price by the stock level for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`pyspark.sql.functions`](http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#module-pyspark.sql.functions) package contains a set of functions that you can apply to data frame columns. The example above where we squared the stock levels can be written as follows as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pow\n",
    "\n",
    "phone_df.withColumn('squared_stock', pow(phone_df['stock'], 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Take a look at the functions that [`pyspark.sql.functions`](http://spark.apache.org/docs/2.3.1/api/python/pyspark.sql.html#module-pyspark.sql.functions) offers. Create a new column called `model_name_length` that will return the length of the name of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import <FILL IN>\n",
    "\n",
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating data\n",
    "An important part of data processing is the ability to combine multiple records. In the DataFrame API this is a two-step process: grouping data, and then applying a function to the data.\n",
    "\n",
    "First we group the data using the [`groupBy`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy) method. `groupBy` can operate on one or multiple columns. It will not actually perform the grouping but create a reference to a [`GroupedData`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = phone_df.groupBy('brand')\n",
    "print(type(grouped_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GroupedData` objects can not be inspected or shown, so the following example will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is grouped we can apply one of the standard aggregation functions on it. They are listed at the [`GroupedData`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) API documentation. These are: `min`, `max`, `mean`, `sum` and `count`. We can apply an aggregation to all columns or to a subset of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum for all columns\n",
    "\n",
    "min_df = grouped_df.min('unit_price')\n",
    "\n",
    "min_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `min(unit_price)` is the name of the new column. We can rename this column with the [`withColumnRenamed`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df.withColumnRenamed('min(unit_price)', 'min_unit_price').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "Compute the maximum  of the `unit_price` per brand and rename the resulting column to `max`.\n",
    "(We assume you can do this in one line. Feel free to adapt the cell and use more lines if you want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "max_df = <FILL IN>\n",
    "max_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate multiple aggregations over our data, we use the [`agg`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData.agg) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the sum of the stock column, and calculate the mean of the unit_price column, in one go\n",
    "sum_df = grouped_df.agg({'stock': 'sum', 'unit_price': 'mean'})\n",
    "\n",
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "In exercise 4 you calculated the expected revenue. Use this information to calculate the total expected revenue using the `agg` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions\n",
    "Spark DataFrame functions are powerful and flexible, but there are times you may want to write some custom logic, or to apply code from other Python packages to column in your data frame.\n",
    "\n",
    "Ordinary Python functions cannot be applied directly to your data frame. Instead, you wrap a Python function with a `@udf` decorator specifying the data type your function will output. Next, use the UDF in an appropriate Spark data frame function, such as `filter` for filtering data, or `withColumn` for generating data.\n",
    "\n",
    "Let's look at a UDF that will check if a string is shorter than six characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf('boolean')\n",
    "def is_short(s):\n",
    "    return len(s) < 6\n",
    "\n",
    "phone_df.filter(is_short('model')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "Write a UDF filter function that takes two parameters: a `model` and `stock` parameter. Write the function such that it will only select rows (return `True`) where the model has 'iPhone' in the name, and the stock is a multiple of two (use Python's `%` modulo operator). Show the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDFs can also generate data for you. They can return anything that fits into a data frame together with the `withColumn` function we saw before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf('string')\n",
    "def to_upper(s):\n",
    "    return s.upper()\n",
    "\n",
    "phone_df.withColumn('uppercase', to_upper('brand')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "Write a function that calculates the expected revenue for each model by multiplying the unit price with the stock. Show the resulting data frame. Lastly, calculate the total expected revenue like you did in exercise 7.\n",
    "\n",
    "**Hint**: what should the return value be, with the stock being an integer and the unit price a double value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining with other data sets\n",
    "Often you want to combine multiple datasets on a shared column. In this example we create an extra table with information about phone manufacturers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [\n",
    "    ('Google', 'USA', 1998, 'Sundar Pichai'),\n",
    "    ('Samsung', 'South Korea', 1938 ,'Oh-Hyun Kwon' ),\n",
    "    ('Apple', 'USA', 1976 ,'Tim Cook')\n",
    "]\n",
    "\n",
    "columns = ['company_name', 'hq_country', 'founding_year', 'ceo']\n",
    "\n",
    "company_df = spark.createDataFrame(companies, columns)\n",
    "company_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two DataFrames, we use the [`join`](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join) method on one of the DataFrames. This method takes two arguments: (1) the other DataFrame and (2) a join relation, providing an expression on what columns to join on from both DataFrames. Here we join the two DataFrames on the brand/company_name columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = phone_df.join(company_df, phone_df['brand'] == company_df['company_name'])\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10\n",
    "Join the company data frame on the phone data frame. Select only the models for which the stock is greater than 7, and of which the company HQ is located in the USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<FILL IN>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
